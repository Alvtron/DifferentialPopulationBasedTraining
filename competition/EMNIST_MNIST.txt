Knowledge Sharing for Population Based Neural Network Training
https://link.springer.com/chapter/10.1007/978-3-030-00111-7_22

EMNIST (split: MNIST) LeNet5 without Knowledge Sharing
mean 99.29±.07  
min 99.14
max 99.46
EMNIST (split: MNIST) LeNet5 with Knowledge Sharing
mean 99.33±.10
min 99.09
max 99.47
EMNIST (split: MNIST) MLP without Knowledge Sharing
mean 98.57±.08
min 98.39
max 98.69
EMNIST (split: MNIST) MLP with Knowledge Sharing
mean 98.73±.09
min 98.47
max 98.91

N = 20 individuals are employed,
within G = 100 generations and the ready-function triggers every 317 iterations,
which results in ≈40 epochs for with batch size of 128.

We implemented the PBT algorithm in Python 3 with PyTorch as our deep learning backend.
Our experiments ran on a DGX-1,
whereby each EA employs its population on 2 (MNIST-like) or 4 (balanced EMNIST) Volta NVIDIA GPUs
with 14 GB VRAM each and either 20 (MNIST-like) or 40 (balanced EMNIST) Intel Xeon E5-2698 CPUs.