Knowledge Sharing for Population Based Neural Network Training
https://link.springer.com/chapter/10.1007/978-3-030-00111-7_22

MNIST LeNet5 without Knowledge Sharing
mean 99.18±.08
min 99.04
max 99.34
MNIST LeNet5 with Knowledge Sharing
mean 99.23±.08
min 98.92
max 99.37
MNIST MLP without Knowledge Sharing
mean 98.30±.09
min 98.06
max 98.46
MNIST MLP with Knowledge Sharing
mean 98.44±.09
min 98.24
max 98.57

population size N is 30,
every 250 iterations the ready-function enters the update loop,
and the popula- tion’s life is G = 40 generations long,
which amounts to ≈12 epochs with a batch size of 64.

We implemented the PBT algorithm in Python 3 with PyTorch as our deep learning backend.
Our experiments ran on a DGX-1,
whereby each EA employs its population on 2 (MNIST-like) or 4 (balanced EMNIST) Volta NVIDIA GPUs
with 14 GB VRAM each and either 20 (MNIST-like) or 40 (balanced EMNIST) Intel Xeon E5-2698 CPUs.